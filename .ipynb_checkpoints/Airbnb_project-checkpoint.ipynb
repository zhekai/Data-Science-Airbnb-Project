{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dive into the Boston and Seattle Airbnb dataset\n",
    "\n",
    "#### CRISP-DM Process\n",
    "\n",
    "**Business Understanding**: Here, we try to understand what's the driving factors that determine the Airbnb rental price for Boston and Seattle housing. In this project, we mainly would like to understand the following three quesitons. \n",
    "\n",
    "1. Is there significant price difference in Airbnb housting between Boston and Seattle ?\n",
    "2. What's the major driving factors to predict airbnb housing price for Boston and Seattle respectively ?\n",
    "3. What are the top factors that people needs to most when they consider Airbnb housing? \n",
    "\n",
    "**Data Understanding**ï¼šDatasets from both Boston and Seattle are investigated before data processing   \n",
    "**Prepare Data**: Data cleanning, Processing NaN inputs  \n",
    "**Data Modeling**: Train the model and find the corresponding parameters to predict housing price  \n",
    "**Model Validation** Test the model and evaluate the effectiveness of the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "listings_Boston = pd.read_csv(\"boston_listings.csv\")\n",
    "listings_Seattle = pd.read_csv(\"seattle_listings.csv\")\n",
    "\n",
    "listings_Boston.drop(['access', 'house_rules', 'interaction'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "#listings_Boston.columns[~listings_Boston.columns.isin(listings_Seattle)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleanup(listings):\n",
    "    '''\n",
    "    input:\n",
    "        listings: The listings dataset of target city.\n",
    "    output:\n",
    "        df2: Cleanup data used for price predition\n",
    "        price: price of the properties.\n",
    "    '''\n",
    "    \n",
    "    #drop some irrevleent\n",
    " \n",
    "    listings.drop(['listing_url', 'scrape_id','last_scraped', 'name', 'summary', 'space','description',\\\n",
    "                   'neighborhood_overview', 'notes','transit', 'thumbnail_url', 'medium_url', 'picture_url', \\\n",
    "                   'xl_picture_url', 'host_url','host_name','host_location', 'host_about','host_thumbnail_url',\\\n",
    "                   'host_picture_url','city', 'state', 'smart_location', 'smart_location', 'country_code', \\\n",
    "                   'country', 'first_review', 'last_review', 'id', 'host_verifications', \\\n",
    "                   'host_id', 'neighbourhood', 'calendar_last_scraped', 'market','street', 'host_since'\\\n",
    "                  ], axis = 1, inplace = True)\n",
    "   \n",
    "\n",
    "\n",
    "    # Dropping NaN with ratio more than 75%\n",
    "    dropping_index = listings.isnull().sum() / listings.shape[0] > 0.75\n",
    "    listings.drop(listings.columns[dropping_index], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "    #takes care some data type issues\n",
    "    listings['host_response_rate'] = listings['host_response_rate'].replace('%','', regex = True).astype(float)\n",
    "    listings['host_acceptance_rate'] = listings['host_acceptance_rate'].replace('%','', regex = True).astype(float)\n",
    "    listings['cleaning_fee'] = listings['cleaning_fee'].replace('\\$', '',regex = True).astype(float)\n",
    "    listings['price'] = listings['price'].replace({'\\$': '', ',':''}, regex = True).astype(float)\n",
    "    listings['extra_people'] = listings['extra_people'].replace({'\\$': '', ',':''}, regex = True).astype(float)\n",
    "    listings['security_deposit'] = listings['security_deposit'].replace({'\\$': '', ',':''}, regex = True).astype(float)\n",
    "\n",
    "    if type(listings.zipcode[0]) == str:\n",
    "            listings.zipcode = listings.zipcode.str[-5:].apply(lambda x: float(x))\n",
    "\n",
    "    listings['amenities'] = listings['amenities'].map(lambda d: [amenity.replace('\"', \"\").replace(\"{\", \"\").replace(\"}\", \"\") for amenity in d.split(\",\")])\n",
    "\n",
    "\n",
    "    possible_amenities = set([item for sublist in listings['amenities'] for item in sublist])\n",
    "    possible_amenities = list(possible_amenities)\n",
    "\n",
    "\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    amenities_result = pd.DataFrame(mlb.fit_transform(listings['amenities']), columns = mlb.classes_, index = listings.index)\n",
    "    listings = pd.concat([listings.drop(['amenities'], axis = 1), amenities_result], axis = 1, sort = False)\n",
    "\n",
    "\n",
    "    cat_column = listings.select_dtypes(include = 'object').columns\n",
    "\n",
    "\n",
    "    for col in cat_column:\n",
    "        dummy_column = pd.get_dummies(listings[col], prefix = col )\n",
    "        listings = pd.concat([listings.drop([col], axis = 1), dummy_column], axis = 1)\n",
    "\n",
    "\n",
    "    numeric_column = listings.select_dtypes(include = ['float', 'int']).columns\n",
    "\n",
    "    #listings_Boston['host_response_rate'].fillna(value = listings_Boston['host_response_rate'].mean())\n",
    "    for col in numeric_column:\n",
    "        listings[col].fillna(value = listings[col].mean(), inplace = True)\n",
    "\n",
    "    y = listings['price']\n",
    "    X = listings.drop(['price'], axis = 1)\n",
    "    return y, X\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_Boston, Boston = data_cleanup(listings_Boston)\n",
    "price_Seattle, Seattle = data_cleanup(listings_Seattle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_training(data, price):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Seattle, price_Seattle, test_size=0.25, random_state=2)\n",
    "    lm_model = LinearRegression(normalize=True)\n",
    "    lm_model.fit(X_train, y_train)\n",
    "    y_test_preds = lm_model.predict(X_test)\n",
    "    y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "    r2_score(y_test, y_test_preds)\n",
    "\n",
    "        # Grid Search to find Best Parameter\n",
    "    param_grid = {'n_estimators': [400], 'learning_rate': [0.05],'max_depth': [10],'min_samples_split': [5],'subsample': [0.7]}\n",
    "\n",
    "    forest_reg = GradientBoostingRegressor(random_state = 42)\n",
    "    grid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring = 'neg_mean_squared_error', return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(grid_search.best_params_)\n",
    "    #cvres = grid_search.cv_results_\n",
    "    #for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    #    print(np.sqrt(-mean_score), params)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "    print(\"Error Score on Test Data: {}.\".format(np.sqrt(sklearn.metrics.mean_squared_error(y_test, y_pred))))\n",
    "    feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "    features = sorted(zip(feature_importances, data.columns), reverse=True)\n",
    "    attribute = []\n",
    "    coefficient = []\n",
    "    for feature in features:\n",
    "        attribute.append(feature[1]) \n",
    "        coefficient.append(feature[0])\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.bar(attribute[:20], height=coefficient[:20])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel('feature')\n",
    "    plt.ylabel('feature importance')\n",
    "    plt.title('feature importance for the Top 20 features')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 400, 'subsample': 0.7}\n",
      "Error Score on Test Data: 48.6831452709256.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Seatle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-0db451b920dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBoston\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice_Boston\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-90374f0b4322>\u001b[0m in \u001b[0;36mdata_training\u001b[0;34m(data, price)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error Score on Test Data: {}.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mfeature_importances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_importances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeatle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mattribute\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mcoefficient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Seatle' is not defined"
     ]
    }
   ],
   "source": [
    "data_training(Boston, price_Boston)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
